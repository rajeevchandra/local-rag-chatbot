# ğŸ¤– Local RAG Chatbot with Ollama, LangChain & ChromaDB

Ask questions about your **own Markdown and PDF documents** â€” 100% locally, with no cloud or API keys required.

Powered by:
- ğŸ§  [Ollama](https://ollama.com) (Mistral + nomic-embed-text)
- ğŸ§± [LangChain](https://www.langchain.com)
- ğŸ“¦ [ChromaDB](https://www.trychroma.com)
- ğŸ’¬ [Streamlit](https://streamlit.io)

---

## ğŸ“¸ Demo

![Chatbot UI Screenshot](https://user-images.githubusercontent.com/placeholder/screenshot.png)

---

## ğŸ” Features

âœ… Upload `.md` and `.pdf` files  
âœ… Automatic re-indexing on upload  
âœ… RAG pipeline using local embeddings  
âœ… Streamlit chatbot interface  
âœ… Multi-turn memory (session context)  
âœ… Source highlighting and content previews  
âœ… Runs on CPU (no GPU required)

---

## ğŸ“¦ Requirements

- Python 3.10+
- Ollama installed locally (https://ollama.com)
- Models pulled:  
  ```bash
  ollama run mistral
  ollama run nomic-embed-text


ğŸ”§ Installation

git clone https://github.com/yourname/local-rag-chatbot.git
cd local-rag-chatbot
pip install -r requirements.txt
pip install "unstructured[pdf]" fpdf

ğŸš€ Run the App

ollama run mistral
streamlit run ui.py

Visit http://localhost:8501 in your browser.

ğŸ“ Folder Structure

ğŸ“¦ rag_ollama_advanced
â”œâ”€â”€ ui.py                # Streamlit chatbot interface
â”œâ”€â”€ ingest_docs.py       # Optional standalone ingestion script
â”œâ”€â”€ ingest_docs_single_model.py
â”œâ”€â”€ rag_pipeline.py      # Embeddings + retriever setup
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docs/                # Place your .md/.pdf files here
â”‚   â”œâ”€â”€ api_design.md
â”‚   â”œâ”€â”€ docker_compose.md
â”‚   â”œâ”€â”€ devops_best_practices.pdf
â”‚   â””â”€â”€ ...
â””â”€â”€ db/                  # ChromaDB persisted vector store


ğŸ’¡ How It Works (RAG Pipeline)
Load .md and .pdf files into LangChain

Split into chunks using RecursiveCharacterTextSplitter

Generate embeddings with Ollamaâ€™s nomic-embed-text

Store embeddings in ChromaDB (./db)

On query, retrieve similar chunks

Inject context into a prompt and send to mistral via Ollama

Display answer + source documents in Streamlit

ğŸ’¬ Example Prompts
"What is a microservice?"
"How does Kubernetes manage pod lifecycle?"
"Give me an example Docker Compose file."
"What are DevOps best practices?"

ğŸ” Private & Secure
No API keys

No cloud calls

All embeddings and LLM runs locally

Ideal for:

Internal developer docs

Security/compliance handbooks

Offline private AI assistants